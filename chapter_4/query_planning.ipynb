{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Monday, December 23, 2024\n",
        "\n",
        "Running 'lmstudio-community/Qwen2.5-14B-Instruct-GGUF' through LMStudio.\n",
        "\n",
        "This all runs in one pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deliberately set the OPENAI_API_KEY to an invalid value to ensure that the code is not using it.\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"Nope!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers.pydantic import PydanticOutputParser\n",
        "from langchain_core.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "from pydantic.v1 import BaseModel, Field\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Query(BaseModel):\n",
        "    id: int\n",
        "    question: str\n",
        "    dependencies: List[int] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"\"\"A list of sub-queries that must be completed before this task can be completed. \n",
        "        Use a sub query when anything is unknown and we might need to ask many queries to get an answer. \n",
        "        Dependencies must only be other queries.\"\"\"\n",
        "    )\n",
        "\n",
        "class QueryPlan(BaseModel):\n",
        "    query_graph: List[Query]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = ChatOpenAI()\n",
        "\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "import json\n",
        "from os import getenv\n",
        "\n",
        "# 2070 Super\n",
        "api_key = \"LMStudio\"\n",
        "lmstudio = \"http://127.0.0.1:1234/v1\"\n",
        "model = \"lmstudio-community/Qwen2.5-14B-Instruct-GGUF\"\n",
        "\n",
        "# chat = ChatOpenAI(base_url=lmstudio, model=model, api_key=\"LMStudio\",  model_kwargs={\"response_format\": {\"type\": \"json_object\"}})\n",
        "model = ChatOpenAI(base_url=lmstudio, api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up a parser:\n",
        "parser = PydanticOutputParser(pydantic_object=QueryPlan)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"Generate a query plan. This will be used for task execution.\n",
        "\n",
        "Answer the following query: {query}\n",
        "\n",
        "Return the following query graph format:\n",
        "{format_instructions}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
        "\n",
        "# Create the LCEL chain with the prompt, model and parser:\n",
        "chain = chat_prompt | model | parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Query(id=1, question='Get the results from the database.', dependencies=[]), Query(id=2, question='Find out what the average age of my top 10 customers is.', dependencies=[1]), Query(id=3, question='Send an email to John with the average age of my top 10 customers.', dependencies=[2]), Query(id=4, question='Send a welcome introduction email to Sarah.', dependencies=[])]\n"
          ]
        }
      ],
      "source": [
        "result = chain.invoke({\n",
        "\"query\":'''I want to get the results from my database. Then I want to find out what the average age of my top 10 customers is.  \n",
        "    Once I have the average age, I want to send an email to John. Also I just generally want to send a welcome introduction email to Sarah, \n",
        "    regardless of the other tasks.''',\n",
        "\"format_instructions\":parser.get_format_instructions()\n",
        "})\n",
        "\n",
        "print(result.query_graph)\n",
        "\n",
        "# 26.2 s\n",
        "\n",
        "# Expected output:\n",
        "# [Query(id=1, question='Get results from database', dependencies=[]), Query(id=2, question='Find average age of top 10 customers', dependencies=[1]), Query(id=3, question='Send email to John', dependencies=[2]), Query(id=4, question='Send welcome introduction email to Sarah', dependencies=[])]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pe4gai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
