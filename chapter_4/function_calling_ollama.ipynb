{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sunday, December 22, 2024\n",
        "\n",
        "Whelp, I am now thinking the reason function calling is failing is because LMStudio does NOT yet support function calling! I am currently using LMStudio 0.3.5 (Build 1). The current [LMStudio Web Site](https://lmstudio.ai/) says 'Function Calling is now in Beta! Try out the [latest build](https://lmstudio.ai/beta-releases)' which as of today is Version 0.3.5 Build 8, last updated on 2024-11-27, so I am gonna download this and give it a go.\n",
        "\n",
        "LMStudio has docs about Tool User [here](https://lmstudio.ai/docs/advanced/tool-use).\n",
        "\n",
        "Hmm so I did download this version, and when I spin it up, it shows 'Build 9'. \n",
        "\n",
        "Nice! Running the code using 'mistral-nemo-instruct-2407', and now I AM getting back the results I want! Great!\n",
        "\n",
        "///////////////////////////////\n",
        "\n",
        "OK! This notebook was copied from 'function_calliing_lmstudio.ipynb' and will be used to test calling a model served up from Ollama. \n",
        "\n",
        "I am running Ollama version 0.5.4 and have run the commana 'ollama run llama3-groq-tool-use'.\n",
        "\n",
        "OK. So using Ollama we get fewer errors than with LMStudio, but even this notebook does not run without errors. Meh for  now ... \n",
        "\n",
        "#### Saturday, December 21, 2024\n",
        "\n",
        "I began this notebook using 'lmstudio-community/Qwen2.5-14B-Instruct-GGUF' but this does not appear to support function calling.\n",
        "OK. Since I so far have not been able to get any local model to perform function calling, I just ran stuff against OpenAI because I wanted to see what DOES work, so then I know how a local model is supposed to work.\n",
        "\n",
        "To see which models on HuggingFace support function calling, go [here](https://huggingface.co/models?other=function-calling)\n",
        "\n",
        "DO NOT RUN THIS AGAIN! I WANT TO KEEP THE OUTPUT FOR FUTURE REFERENCE!\n",
        "\n",
        "OK! So this notebook was copied from 'function_calling.ipynb' and will be used to try and find some local model that supports function calling. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deliberately set the OPENAI_API_KEY to an invalid value to ensure that the code is not using it.\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"Nope!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OpenAI Function Calling with Tool Usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I grabbed this code in the next cell from the ollama docs found [here](https://github.com/ollama/ollama/blob/main/docs/openai.md)\n",
        "\n",
        "For this cell to run without errors, you need to have already downloaded to Ollama the models 'llama3.2','llava' and 'all-minilm'. \n",
        "\n",
        "The fact that you can switch from one ollama model to another at run time is pretty impressive! Can LMStudio do this??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url='http://localhost:11434/v1/',\n",
        "\n",
        "    # required but ignored\n",
        "    api_key='ollama',\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': 'Say this is a test',\n",
        "        }\n",
        "    ],\n",
        "    model='llama3.2',\n",
        ")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"llava\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": \"data:image/png;base64,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\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    max_tokens=300,\n",
        ")\n",
        "\n",
        "completion = client.completions.create(\n",
        "    model=\"llama3.2\",\n",
        "    prompt=\"Say this is a test\",\n",
        ")\n",
        "\n",
        "list_completion = client.models.list()\n",
        "\n",
        "model = client.models.retrieve(\"llama3.2\")\n",
        "\n",
        "embeddings = client.embeddings.create(\n",
        "    model=\"all-minilm\",\n",
        "    input=[\"why is the sky blue?\", \"why is the grass green?\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this if you do actually want to use OpenAI, and of course, do not set the key to something bogus ... \n",
        "# from openai import OpenAI\n",
        "# import json\n",
        "# from os import getenv\n",
        "# client = OpenAI(api_key=getenv(\"OPENAI_API_KEY\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "from os import getenv\n",
        "\n",
        "# client = OpenAI(api_key=getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# 4090 ... \n",
        "\n",
        "# api_key = \"LMStudio\"\n",
        "api_key = \"Ollama\"\n",
        "# lmstudio = \"http://192.168.2.16:1234/v1\"\n",
        "# lmstudio = \"http://127.0.0.1:1234/v1\"\n",
        "ollama = \"http://localhost:11434/v1/\"\n",
        "# model = \"qwen2.5-14b-instruct@q8_0\" # lmstudio-community/Qwen2.5-14B-Instruct-GGUF :  Qwen2.5-14B-Instruct-Q8_0.gguf\n",
        "# model = \"qwen2.5-14b-instruct@q4_k_m\" # lmstudio-community/Qwen2.5-14B-Instruct-GGUF :  Qwen2.5-14B-Instruct-Q4_K_M.gguf\n",
        "# model = \"lmstudio-community/mistral-nemo-instruct-2407\"   # lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF : Mistral-Nemo-Instruct-2407-Q4_K_M.gguf\n",
        "\n",
        "# model = \"lmstudio-community/Llama-3-Groq-8B-Tool-Use-GGUF\"\n",
        "\n",
        "# model = \"lmstudio-community/Mistral-Nemo-Instruct-2407-GGUF\"\n",
        "\n",
        "model = \"llama3-groq-tool-use\"\n",
        "\n",
        "# chat = ChatOpenAI(base_url=lmstudio, model=model, api_key=\"LMStudio\",  model_kwargs={\"response_format\": {\"type\": \"json_object\"}})\n",
        "client = OpenAI(base_url=ollama, api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def schedule_meeting(date, time, attendees):\n",
        "    # Connect to calendar service:\n",
        "    return { \"event_id\": \"1234\", \"status\": \"Meeting scheduled successfully!\",\n",
        "            \"date\": date, \"time\": time, \"attendees\": attendees }\n",
        "\n",
        "OPENAI_FUNCTIONS = {\n",
        "    \"schedule_meeting\": schedule_meeting\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Our predefined function JSON schema:\n",
        "functions = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"type\": \"object\",\n",
        "            \"name\": \"schedule_meeting\",\n",
        "            \"description\": \"Set a meeting at a specified date and time for designated attendees\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"date\": {\"type\": \"string\", \"format\": \"date\"},\n",
        "                    \"time\": {\"type\": \"string\", \"format\": \"time\"},\n",
        "                    \"attendees\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                },\n",
        "                \"required\": [\"date\", \"time\", \"attendees\"],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the conversation:\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob.\",\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send the conversation and function schema to the model:\n",
        "response = client.chat.completions.create(\n",
        "    # model=\"gpt-3.5-turbo-1106\",\n",
        "    # model=\"gpt-3.5-turbo\",\n",
        "    model = model ,\n",
        "    messages=messages,\n",
        "    tools=functions,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "\n",
        "# 7.0s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_x1y88cw8', function=Function(arguments='{\"attendees\":[\"Alice\",\"Bob\"],\"date\":\"2023-11-01\",\"time\":\"14:00\"}', name='schedule_meeting'), type='function', index=0)])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = response.choices[0].message\n",
        "response\n",
        "\n",
        "# response from OpenAI\n",
        "# ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_SPRvNtyTO6AWGqK85FMx2taW', function=Function(arguments='{\"date\": \"2023-11-01\", \"time\": \"14:00\", \"attendees\": [\"Alice\", \"Bob\"]}', name='schedule_meeting'), type='function')], refusal=None)\n",
        "# ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_pHhAr3KfObP6gPzaPNUc9oeK', function=Function(arguments='{\"date\":\"2023-11-01\",\"time\":\"14:00\",\"attendees\":[\"Alice\",\"Bob\"]}', name='schedule_meeting'), type='function')], refusal=None)\n",
        "# ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_HRO0PEi7GG42M81rC5CYLRTq', function=Function(arguments='{\"date\":\"2023-11-01\",\"time\":\"14:00\",\"attendees\":[\"Alice\",\"Bob\"]}', name='schedule_meeting'), type='function')], refusal=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ChatCompletionMessageToolCall(id='call_x1y88cw8', function=Function(arguments='{\"attendees\":[\"Alice\",\"Bob\"],\"date\":\"2023-11-01\",\"time\":\"14:00\"}', name='schedule_meeting'), type='function', index=0)]\n"
          ]
        }
      ],
      "source": [
        "print(response.tool_calls)\n",
        "\n",
        "# response from OpenAI\n",
        "# [ChatCompletionMessageToolCall(id='call_SPRvNtyTO6AWGqK85FMx2taW', function=Function(arguments='{\"date\": \"2023-11-01\", \"time\": \"14:00\", \"attendees\": [\"Alice\", \"Bob\"]}', name='schedule_meeting'), type='function')]\n",
        "# [ChatCompletionMessageToolCall(id='call_pHhAr3KfObP6gPzaPNUc9oeK', function=Function(arguments='{\"date\":\"2023-11-01\",\"time\":\"14:00\",\"attendees\":[\"Alice\",\"Bob\"]}', name='schedule_meeting'), type='function')]\n",
        "# [ChatCompletionMessageToolCall(id='call_HRO0PEi7GG42M81rC5CYLRTq', function=Function(arguments='{\"date\":\"2023-11-01\",\"time\":\"14:00\",\"attendees\":[\"Alice\",\"Bob\"]}', name='schedule_meeting'), type='function')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the function name:  schedule_meeting\n",
            "These are the function arguments:  {'attendees': ['Alice', 'Bob'], 'date': '2023-11-01', 'time': '14:00'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if the model wants to call our function:\n",
        "if response.tool_calls:\n",
        "    # Get the first function call:\n",
        "    first_tool_call = response.tool_calls[0]\n",
        "\n",
        "    # Find the function name and function args to call:\n",
        "    function_name = first_tool_call.function.name\n",
        "    function_args = json.loads(first_tool_call.function.arguments)\n",
        "    print(\"This is the function name: \", function_name)\n",
        "    print(\"These are the function arguments: \", function_args)\n",
        "\n",
        "    function = OPENAI_FUNCTIONS.get(function_name)\n",
        "\n",
        "    if not function:\n",
        "        raise Exception(f\"Function {function_name} not found.\")\n",
        "\n",
        "    # Call the function:\n",
        "    function_response = function(**function_args)\n",
        "\n",
        "    # Share the function's response with the model:\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"function\",\n",
        "            \"name\": \"schedule_meeting\",\n",
        "            \"content\": json.dumps(function_response),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Let the model generate a user-friendly response:\n",
        "    # second_response = client.chat.completions.create(\n",
        "    #     model=\"gpt-3.5-turbo-0613\", messages=messages\n",
        "    # )\n",
        "    # second_response = client.chat.completions.create(\n",
        "    #     model=\"gpt-3.5-turbo\", messages=messages\n",
        "    # )\n",
        "\n",
        "    second_response = client.chat.completions.create(\n",
        "        model=model, messages=messages\n",
        "    )\n",
        "\n",
        "    print(second_response.choices[0].message.content)\n",
        "\n",
        "\n",
        "# Output from 'gpt-3.5-turbo'\n",
        "# This is the function name:  schedule_meeting\n",
        "# These are the function arguments:  {'date': '2023-11-01', 'time': '14:00', 'attendees': ['Alice', 'Bob']}\n",
        "# Meeting scheduled for November 1, 2023 at 14:00 with Alice and Bob. Event ID: 1234.\n",
        "\n",
        "\n",
        "# This is the expected output ...\n",
        "\n",
        "# This is the function name:  schedule_meeting\n",
        "# These are the function arguments:  {'date': '2023-11-01', 'time': '14:00', 'attendees': ['Alice', 'Bob']}\n",
        "# Meeting Scheduled:\n",
        "\n",
        "# Date: 2023-11-01\n",
        "# Time: 14:00\n",
        "# Attendees: Alice, Bob\n",
        "\n",
        "# Please mark your calendars accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above error message is complaining about the content of 'messages', so let's have a look at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'user',\n",
              "  'content': 'Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob.'},\n",
              " {'role': 'function',\n",
              "  'name': 'schedule_meeting',\n",
              "  'content': '{\"event_id\": \"1234\", \"status\": \"Meeting scheduled successfully!\", \"date\": \"2023-11-01\", \"time\": \"14:00\", \"attendees\": [\"Alice\", \"Bob\"]}'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------------------------\n",
        "\n",
        "## Parallel Function Calling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the conversation:\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": '''Schedule a meeting on 2023-11-01 at 14:00 with Alice and Bob. \n",
        "        Then I want to schedule another meeting on 2023-11-02 at 15:00 with Charlie and Dave.'''\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send the conversation and function schema to the model:\n",
        "response = client.chat.completions.create(\n",
        "    # model=\"gpt-3.5-turbo-1106\",\n",
        "    # model=\"gpt-3.5-turbo\",\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    tools=functions,\n",
        ")\n",
        "\n",
        "# 12.1 s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='', role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_426kcmra', function=Function(arguments='{\"attendees\":[\"Alice\",\"Bob\"],\"date\":\"2023-11-01\",\"time\":\"14:00\"}', name='schedule_meeting'), type='function', index=0)])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = response.choices[0].message\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is the function name:  schedule_meeting\n",
            "These are the function arguments:  {'attendees': ['Alice', 'Bob'], 'date': '2023-11-01', 'time': '14:00'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check if the model wants to call our function:\n",
        "if response.tool_calls:\n",
        "    for tool_call in response.tool_calls:\n",
        "        # Get the function name and arguments to call:\n",
        "        function_name = tool_call.function.name\n",
        "        function_args = json.loads(tool_call.function.arguments)\n",
        "        print(\"This is the function name: \", function_name)\n",
        "        print(\"These are the function arguments: \", function_args)\n",
        "\n",
        "        function = OPENAI_FUNCTIONS.get(function_name)\n",
        "\n",
        "        if not function:\n",
        "            raise Exception(f\"Function {function_name} not found.\")\n",
        "\n",
        "        # Call the function:\n",
        "        function_response = function(**function_args)\n",
        "\n",
        "        # Share the function's response with the model:\n",
        "        messages.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": json.dumps(function_response),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Let the model generate a user-friendly response:\n",
        "    # second_response = client.chat.completions.create(\n",
        "    #     model=\"gpt-3.5-turbo-0613\", messages=messages\n",
        "    # )\n",
        "    # second_response = client.chat.completions.create(\n",
        "    #     model=\"gpt-3.5-turbo\", messages=messages\n",
        "    # )\n",
        "    second_response = client.chat.completions.create(\n",
        "        model=model, messages=messages\n",
        "    )\n",
        "\n",
        "    print(second_response.choices[0].message.content)\n",
        "\n",
        "# output from 'gpt-3.5-turbo' ...\n",
        "# This is the function name:  schedule_meeting\n",
        "# These are the function arguments:  {'date': '2023-11-01', 'time': '14:00', 'attendees': ['Alice', 'Bob']}\n",
        "# This is the function name:  schedule_meeting\n",
        "# These are the function arguments:  {'date': '2023-11-02', 'time': '15:00', 'attendees': ['Charlie', 'Dave']}\n",
        "# Two meetings have been successfully scheduled:\n",
        "# 1. Meeting with Alice and Bob on 2023-11-01 at 14:00\n",
        "# 2. Meeting with Charlie and Dave on 2023-11-02 at 15:00."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Function Calling in LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Article(BaseModel):\n",
        "    \"\"\"Identifying key points and contrarian views in an article.\"\"\"\n",
        "\n",
        "    points: str = Field(..., description=\"Key points from the article\")\n",
        "    contrarian_points: Optional[str] = Field(\n",
        "        None, description=\"Any contrarian points acknowledged in the article\"\n",
        "    )\n",
        "    author: Optional[str] = Field(None, description=\"Author of the article\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\\n",
        "in the following passage together with their properties.\n",
        "\n",
        "If a property is not present and is not required in the function parameters, do not include it in the output.\"\"\"\n",
        "\n",
        "# Create a prompt telling the LLM to extract information\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    {(\"system\", _EXTRACTION_TEMPLATE), (\"user\", \"{input}\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'OpenAI' object has no attribute 'bind_tools'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m tools \u001b[38;5;241m=\u001b[39m [convert_to_openai_tool(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pydantic_schemas]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Give the model access to these tools:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model = model.bind_tools(tools=tools)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m(tools\u001b[38;5;241m=\u001b[39mtools)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create an end to end chain\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m client \u001b[38;5;241m|\u001b[39m PydanticToolsParser(tools\u001b[38;5;241m=\u001b[39mpydantic_schemas)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'OpenAI' object has no attribute 'bind_tools'"
          ]
        }
      ],
      "source": [
        "pydantic_schemas = [Article]\n",
        "\n",
        "# Convert Pydantic objects to the appropriate schema:\n",
        "tools = [convert_to_openai_tool(p) for p in pydantic_schemas]\n",
        "\n",
        "# Give the model access to these tools:\n",
        "# model = model.bind_tools(tools=tools)\n",
        "client = client.bind_tools(tools=tools)\n",
        "\n",
        "# Create an end to end chain\n",
        "# chain = prompt | model | PydanticToolsParser(tools=pydantic_schemas)\n",
        "chain = prompt | client | PydanticToolsParser(tools=pydantic_schemas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Article(points='Key points addressed include the growing interest in AI in various sectors, the increase in AI research, and the need for responsible AI.', contrarian_points='Without stringent regulations, AI may pose high risks.', author='Dr. Jane Smith')]\n"
          ]
        }
      ],
      "source": [
        "result = chain.invoke(\n",
        "    {\n",
        "        \"input\": \"\"\"In the recent article titled 'AI adoption in industry', key points addressed include the growing interest in AI in various sectors, the increase in AI research, and the need for responsible AI. However, the author, Dr. Jane Smith, acknowledges a contrarian view — that without stringent regulations, AI may pose high risks.\"\"\"\n",
        "    }\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# output from openai ...\n",
        "# [Article(points='Key points addressed include the growing interest in AI in various sectors, the increase in AI research, and the need for responsible AI.', contrarian_points='Without stringent regulations, AI may pose high risks.', author='Dr. Jane Smith')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Extracting Chains with LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chains.openai_tools import create_extraction_chain_pydantic\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make sure to use a recent model that supports tools\n",
        "# model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Person(BaseModel):\n",
        "    \"\"\"A person's name and age.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The person's name\")\n",
        "    age: int = Field(..., description=\"The person's age\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Person(name='Bob', age=25), Person(name='Sarah', age=30)]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = create_extraction_chain_pydantic(Person, model)\n",
        "chain.invoke({'input':'''Bob is 25 years old. He lives in New York. He likes to play basketball. Sarah is 30 years old. She lives in San Francisco. She likes to play tennis.'''})\n",
        "\n",
        "# output from openai ...\n",
        "# [Person(name='Bob', age=25), Person(name='Sarah', age=30)]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pe4gai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
