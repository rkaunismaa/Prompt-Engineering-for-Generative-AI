{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saturday, January 11, 2025\n",
    "\n",
    "Running again but using the 4090 on KAUWITB, running LMStudio serving 'lmstudio-community/Qwen2.5-14B-Instruct-GGUF' (Qwen2.5-14B-Instruct-Q4_K_M.gguf)\n",
    "\n",
    "#### Wednesday, December 18, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliberately set the OPENAI_API_KEY to an invalid value to ensure that the code is not using it.\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"Nope!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "lmstudio = \"http://192.168.2.16:1234/v1\"\n",
    "\n",
    "client = OpenAI(base_url=lmstudio, api_key=\"lm-studio\")\n",
    "\n",
    "# model = \"qwen2.5-14b-instruct@q8_0\" # lmstudio-community/Qwen2.5-14B-Instruct-GGUF :  Qwen2.5-14B-Instruct-Q8_0.gguF\n",
    "\n",
    "model = \"Qwen2.5-14B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "temperature = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_template = \"\"\"\n",
    "Given the statement, classify it as either \"Compliment\", \"Complaint\", or \"Neutral\":\n",
    "1. \"The sun is shining.\" - Neutral\n",
    "2. \"Your support team is fantastic!\" - Compliment\n",
    "3. \"I had a terrible experience with your software.\" - Complaint\n",
    "\n",
    "You must follow the following principles:\n",
    "- Only return the single classification word. The response should be either \"Compliment\", \"Complaint\" or \"Neutral\".\n",
    "- Perform the classification on the text enclosed within ''' delimiters.\n",
    "\n",
    "'''{content}'''\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for i in range(0, 3):\n",
    "    response = client.chat.completions.create(\n",
    "        # model=\"gpt-4\",\n",
    "        model = model,\n",
    "        temperature = temperature, \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": base_template.format(\n",
    "                    content=\"Outside is rainy, but I am having a great day, I just don't understand how people live, I'm so sad!\"\n",
    "                ),\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    responses.append(response.choices[0].message.content.strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_classification(responses):\n",
    "    # Use a dictionary to count occurrences of each classification\n",
    "    count_dict = {}\n",
    "    for classification in responses:\n",
    "        count_dict[classification] = count_dict.get(classification, 0) + 1\n",
    "\n",
    "    # Return the classification with the maximum count\n",
    "    return max(count_dict, key=count_dict.get)\n",
    "\n",
    "print(most_frequent_classification(responses))  # Expected Output: Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is supposed to be 'Neutral' but yet we get back 'Complaint'. Would we get a different response with a different local model?\n",
    "\n",
    "I tried a bunch of different models, and below are the ones that returned one of the desired outputs:\n",
    "\n",
    "* lmstudio-community/granite-3.0-8b-instruct-GGUF => 'Compliment'\n",
    "* models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF => 'Neutral'\n",
    "* models/lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF => 'Compliment'\n",
    "* NousResearch/Hermes-3-Llama-3.1-8B-GGUF => 'Complaint'\n",
    "* models/microsoft/Phi-3-mini-4k-instruct-gguf => 'Complaint'\n",
    "* mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated-GGUF => 'Complaint'\n",
    "* models/TheBloke/Nous-Hermes-2-SOLAR-10.7B-GGUF => 'Complaint'\n",
    "* models/PrunaAI/dolphin-2.9-llama3-8b-256k-GGUF-smashed => 'Compliment'\n",
    "* models/TheBloke/dolphin-2.5-mixtral-8x7b-GGUF => 'Neutral'\n",
    "* models/TheBloke/dolphin-2.6-mistral-7B-GGUF => \"Neutral'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pe4gai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
